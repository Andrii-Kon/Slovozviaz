import openai
import json
import os
import numpy as np
import time
from tqdm import tqdm  # —ñ–º–ø–æ—Ä—Ç—É—î–º–æ tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É
from dotenv import load_dotenv

load_dotenv()

# –ó—á–∏—Ç—É—î–º–æ –∫–ª—é—á —ñ–∑ –∑–º—ñ–Ω–Ω–æ—ó –æ—Ç–æ—á–µ–Ω–Ω—è
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–º—ñ–Ω–Ω–æ—ó –æ—Ç–æ—á–µ–Ω–Ω—è OPENAI_API_KEY!")

client = openai.OpenAI(api_key=api_key)

CACHE_FILE = "embeddings_cache.json"
BATCH_SIZE = 100  # —Ä–æ–∑–º—ñ—Ä –ø–∞–∫–µ—Ç—É

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è/—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–µ—à—É –µ–º–±–µ–¥–¥—ñ–Ω–≥—ñ–≤
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        embedding_cache = json.load(f)
else:
    embedding_cache = {}

def get_embeddings_batch(phrases, model="text-embedding-3-small"):
    # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ, —â–æ –≤–∂–µ –≤ –∫–µ—à—ñ
    uncached = [p for p in phrases if p not in embedding_cache]

    if uncached:
        for i in range(0, len(uncached), BATCH_SIZE):
            batch = uncached[i:i + BATCH_SIZE]
            print(f"üîÑ –ù–∞–¥—Å–∏–ª–∞—î–º–æ batch {i + 1}‚Äì{i + len(batch)}")
            response = client.embeddings.create(
                input=batch,
                model=model
            )
            for phrase, obj in zip(batch, response.data):
                embedding_cache[phrase] = obj.embedding

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ –∫–µ—à—É
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(embedding_cache, f, ensure_ascii=False)

    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –µ–º–±–µ–¥–¥—ñ–Ω–≥–∏ —É —Ç–æ–º—É –ø–æ—Ä—è–¥–∫—É, —è–∫ –ø–µ—Ä–µ–¥–∞–Ω–æ
    return [embedding_cache[p] for p in phrases]

def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

if __name__ == "__main__":
    start_time = time.time()

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∏–∑–Ω–∞—á–µ–Ω—å –∑—ñ —Ñ–∞–π–ª—É definitions.json
    with open("definitions.json", "r", encoding="utf-8") as f:
        definitions = json.load(f)

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ü—ñ–ª—å–æ–≤–µ —Å–ª–æ–≤–æ —Ç–∞ –π–æ–≥–æ —Ñ—Ä–∞–∑—É (–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è)
    target_word = "—ñ—Å—Ç–æ—Ä—ñ—è"
    target_phrase = definitions.get(target_word, target_word)
    print(f"–û—Ç—Ä–∏–º—É—î–º–æ embedding –¥–ª—è —Ñ—Ä–∞–∑–∏: {target_phrase}")
    target_emb = get_embeddings_batch([target_phrase])[0]

    # –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —Å–ª—ñ–≤ –∑ —Ñ–∞–π–ª—É wordlist.txt
    with open("wordlist.txt", "r", encoding="utf-8") as f:
        words = [w.strip() for w in f if w.strip()]

    # –§–æ—Ä–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑: —è–∫—â–æ —î –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ, —ñ–Ω–∞–∫—à–µ —Å–∞–º–µ —Å–ª–æ–≤–æ
    phrases = [definitions.get(word, word) for word in words]

    print("üì• –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥–¥—ñ–Ω–≥–∏ –¥–ª—è –≤—Å—ñ—Ö —Ñ—Ä–∞–∑...")
    embeddings = get_embeddings_batch(phrases)

    # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Å—Ö–æ–∂–æ—Å—Ç—ñ –∑ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä–æ–º
    temp_list = []
    for word, emb in tqdm(zip(words, embeddings), total=len(words), desc="–û–±—Ä–æ–±–∫–∞ —Å–ª—ñ–≤"):
        sim = cosine_similarity(emb, target_emb)
        temp_list.append((word, sim))

    # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —Å–ø–∞–¥–∞–Ω–Ω—è–º —Å—Ö–æ–∂–æ—Å—Ç—ñ
    temp_list.sort(key=lambda x: x[1], reverse=True)

    ranked_words = [
        {"word": w, "similarity": sim, "rank": rank}
        for rank, (w, sim) in enumerate(temp_list, start=1)
    ]

    with open("ranked_words.json", "w", encoding="utf-8") as f:
        json.dump(ranked_words, f, ensure_ascii=False, indent=2)

    print("‚úÖ –§–∞–π–ª ranked_words.json —Å—Ç–≤–æ—Ä–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
    elapsed_time = time.time() - start_time
    print(f"–ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
