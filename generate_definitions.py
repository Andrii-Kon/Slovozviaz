import openai
import json
import os
import re
import time
import concurrent.futures
import threading
from dotenv import load_dotenv

load_dotenv()

# –í–∏–≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ—ó —Ä–æ–±–æ—á–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
print("–ü–æ—Ç–æ—á–Ω–∞ —Ä–æ–±–æ—á–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è:", os.getcwd())

client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

INPUT_FILE = "wordlist.txt"
OUTPUT_FILE = "definitions.json"

# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ —ñ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è
definitions = {}
lock = threading.Lock()

# –Ø–∫—â–æ —Ñ–∞–π–ª —ñ—Å–Ω—É—î, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –π–æ–≥–æ
if os.path.exists(OUTPUT_FILE) and os.path.getsize(OUTPUT_FILE) > 0:
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            definitions = json.load(f)
    except json.decoder.JSONDecodeError:
        print("‚ö†Ô∏è  –§–∞–π–ª definitions.json –º–∞—î –Ω–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Å–ª–æ–≤–Ω–∏–∫.")
        definitions = {}

with open(INPUT_FILE, "r", encoding="utf-8") as f:
    words = [line.strip() for line in f if line.strip()]

def remove_word_repetition(word, definition):
    """
    –í–∏–¥–∞–ª—è—î –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è —Å–∞–º–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–∞ –ø–æ—á–∞—Ç–∫—É –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è.
    –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –¥–ª—è —Å–ª–æ–≤–∞ "—Ä—ñ–∫" –≤–∏–¥–∞–ª—è—î "—Ä—ñ–∫ ‚Äî", "—Ä—ñ–∫:" –∞–±–æ "—Ä—ñ–∫ -".
    """
    pattern = re.compile(rf"^\s*{re.escape(word)}\s*(‚Äî|-|:)?\s*", re.IGNORECASE)
    return pattern.sub("", definition, count=1)

def save_definition(word, definition):
    """
    –î–æ–¥–∞—î –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞ —Ç–∞ –∑–∞–ø–∏—Å—É—î —É —Ñ–∞–π–ª.
    –û–≥–æ—Ä–Ω—É—Ç–æ –≤ lock, —â–æ–± –æ–¥–Ω–æ—á–∞—Å–Ω–∏–π –¥–æ—Å—Ç—É–ø –Ω–µ –∑—ñ–ø—Å—É–≤–∞–≤ —Ñ–∞–π–ª.
    """
    with lock:
        definitions[word] = definition
        # –ó–∞–ø–∏—Å—É—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ —É —Ñ–∞–π–ª
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(definitions, f, ensure_ascii=False, indent=2)

def generate_definition(word, max_retries=5):
    prompt = (
        f"–î–∞–π –∫–æ—Ä–æ—Ç–∫–µ, –ª–∞–∫–æ–Ω—ñ—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Å–ª–æ–≤–∞ '{word}' —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é –≤ –æ–¥–Ω–æ–º—É —Ä–µ—á–µ–Ω–Ω—ñ. "
        "–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ–≤–∏–Ω–Ω–æ —Ç–æ—á–Ω–æ –æ–ø–∏—Å—É–≤–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–µ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è —Å–ª–æ–≤–∞, –π–æ–≥–æ –∫–ª—é—á–æ–≤—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, "
        "–±–µ–∑ –∑–∞–π–≤–∏—Ö –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫—ñ–≤ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –¥–µ—Ç–∞–ª–µ–π. –ù–µ –ø–æ–≤—Ç–æ—Ä—é–π —Å–∞–º–µ —Å–ª–æ–≤–æ –Ω–∞ –ø–æ—á–∞—Ç–∫—É –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è."
    )
    attempt = 0
    while attempt < max_retries:
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": (
                        "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑—ñ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–∏—Ö, –ª–∞–∫–æ–Ω—ñ—á–Ω–∏—Ö –≤–∏–∑–Ω–∞—á–µ–Ω—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. "
                        "–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –¥–∞–≤–∞—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, —â–æ —Ç–æ—á–Ω–æ –æ–ø–∏—Å—É—é—Ç—å –æ—Å–Ω–æ–≤–Ω–µ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è —Å–ª–æ–≤–∞ –≤ –æ–¥–Ω–æ–º—É —Ä–µ—á–µ–Ω–Ω—ñ. "
                        "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ø—Ä–æ—Å—Ç—É —ñ –∑—Ä–æ–∑—É–º—ñ–ª—É –º–æ–≤—É, –±–µ–∑ –∑–∞–π–≤–∏—Ö –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫—ñ–≤ —ñ –æ–ø–∏—Å–æ–≤–∏—Ö –≤—ñ–¥—Ç—ñ–Ω–∫—ñ–≤. "
                        "–ù–µ –ø–æ–≤—Ç–æ—Ä—é–π —Å–ª–æ–≤–æ, –¥–ª—è —è–∫–æ–≥–æ –≥–µ–Ω–µ—Ä—É—î—Ç—å—Å—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –Ω–∞ –ø–æ—á–∞—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."
                    )},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=100  # –∑–±—ñ–ª—å—à–µ–Ω–æ –∑ 50 –¥–æ 100
            )
            raw_definition = response.choices[0].message.content.strip()
            clean_definition = remove_word_repetition(word, raw_definition)
            print(f"üìå '{word}': {clean_definition}")
            return word, clean_definition

        except Exception as e:
            if "rate_limit_exceeded" in str(e):
                wait_time = 10 * (2 ** attempt)  # –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–∏–π backoff
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥–ª—è '{word}': {e}. –ß–µ–∫–∞—î–º–æ {wait_time} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ—é —Å–ø—Ä–æ–±–æ—é.")
                time.sleep(wait_time)
                attempt += 1
            else:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥–ª—è '{word}': {e}. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ü–µ —Å–ª–æ–≤–æ.")
                return word, None

    print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è '{word}' –ø—ñ—Å–ª—è {max_retries} —Å–ø—Ä–æ–±.")
    return word, None

def main():
    to_process = [word for word in words if word not in definitions]
    print(f"–ó–∞–ª–∏—à–∏–ª–æ—Å—å –æ–±—Ä–æ–±–∏—Ç–∏: {len(to_process)} —Å–ª—ñ–≤.")
    start_time = time.time()

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(generate_definition, word): word for word in to_process}
        for future in concurrent.futures.as_completed(futures):
            word, definition = future.result()
            if definition:
                save_definition(word, definition)

    end_time = time.time()
    print(f"\n–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤–∏–∑–Ω–∞—á–µ–Ω—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥.")

if __name__ == "__main__":
    main()