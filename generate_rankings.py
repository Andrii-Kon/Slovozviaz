import openai
import json
import os
import numpy as np
import time
from tqdm import tqdm
from dotenv import load_dotenv
from datetime import timedelta, date

load_dotenv()

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∫–ª—é—á–∞ API –≤ –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–º—ñ–Ω–Ω–æ—ó –æ—Ç–æ—á–µ–Ω–Ω—è OPENAI_API_KEY!")

client = openai.OpenAI(api_key=api_key)

# –®–ª—è—Ö –¥–æ –∫–µ—à—É –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏
CACHE_FILE = "embeddings_cache.json"
BATCH_SIZE = 100
BASE_DATE = date(2025, 6, 2)

# –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –æ–±—á–∏—Å–ª–µ–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
if not os.path.exists("precomputed"):
    os.makedirs("precomputed")

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—å–æ–≥–æ
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        embedding_cache = json.load(f)
else:
    embedding_cache = {}

def get_embeddings_batch(phrases, model="text-embedding-3-large"):
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –µ–º–±–µ–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫—É —Ñ—Ä–∞–∑, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –ª–æ–∫–∞–ª—å–Ω–∏–π –∫–µ—à.
    –ù–æ–≤—ñ —Ñ—Ä–∞–∑–∏ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—é—Ç—å—Å—è –≤ API –ø–∞–∫–µ—Ç–∞–º–∏ —Ä–æ–∑–º—ñ—Ä–æ–º BATCH_SIZE.
    """
    uncached = [p for p in phrases if p not in embedding_cache]

    if uncached:
        for i in range(0, len(uncached), BATCH_SIZE):
            batch = uncached[i:i + BATCH_SIZE]
            print(f"–ù–∞–¥—Å–∏–ª–∞—î–º–æ batch {i + 1}‚Äì{i + len(batch)}")
            response = client.embeddings.create(input=batch, model=model)
            for phrase, obj in zip(batch, response.data):
                embedding_cache[phrase] = obj.embedding

        # –û–Ω–æ–≤–ª—é—î–º–æ –∫–µ—à –Ω–∞ –¥–∏—Å–∫—É –ø—ñ—Å–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(embedding_cache, f, ensure_ascii=False)

    return [embedding_cache[p] for p in phrases]

def cosine_similarity(a, b):
    """
    –ö–æ—Å–∏–Ω—É—Å–Ω–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –º—ñ–∂ –¥–≤–æ–º–∞ –≤–µ–∫—Ç–æ—Ä–∞–º–∏.
    """
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def generate_rankings(target_word, target_date, definitions, words):
    """
    –ë—É–¥—É—î —Ä–µ–π—Ç–∏–Ω–≥ —Å—Ö–æ–∂–æ—Å—Ç—ñ –≤—Å—ñ—Ö —Å–ª—ñ–≤ –∑—ñ —Å–ø–∏—Å–∫—É `words` –¥–æ —Ü—ñ–ª—å–æ–≤–æ–≥–æ `target_word`.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞ `definitions` —è–∫ —Ç–µ–∫—Å—Ç –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤,
    —è–∫—â–æ –≤–æ–Ω–∏ –Ω–∞–¥–∞–Ω—ñ, —ñ–Ω–∞–∫—à–µ ‚Äî —Å–∞–º–µ —Å–ª–æ–≤–æ.
    –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è —É —Ñ–∞–π–ª—ñ precomputed/{target_date}.json.
    """
    print(f"[{target_date}] –û–±—Ä–æ–±–∫–∞ —Å–ª–æ–≤–∞: {target_word}")
    target_phrase = definitions.get(target_word, target_word)
    target_emb = get_embeddings_batch([target_phrase])[0]

    phrases = [definitions.get(w, w) for w in words]
    print("üì• –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥–¥—ñ–Ω–≥–∏ –¥–ª—è –≤—Å—ñ—Ö —Ñ—Ä–∞–∑...")
    embeddings = get_embeddings_batch(phrases)

    scored = []
    for word, emb in tqdm(zip(words, embeddings), total=len(words), desc="–û–±—á–∏—Å–ª—é—î–º–æ —Å—Ö–æ–∂—ñ—Å—Ç—å"):
        sim = cosine_similarity(emb, target_emb)
        scored.append((word, sim))

    # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —Å–ø–∞–¥–∞–Ω–Ω—è–º —Å—Ö–æ–∂–æ—Å—Ç—ñ —Ç–∞ –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è —Ä–∞–Ω–≥—É
    scored.sort(key=lambda x: x[1], reverse=True)
    ranked_words = [{"word": w, "similarity": s, "rank": r} for r, (w, s) in enumerate(scored, start=1)]

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –¥–∞—Ç–∏
    filename = f"precomputed/{target_date}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(ranked_words, f, ensure_ascii=False, indent=2)

    print(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ —É {filename} ({len(ranked_words)} —Å–ª—ñ–≤)")
    return ranked_words

if __name__ == "__main__":
    # –ü–∞–∫–µ—Ç–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–ª—è –≤—Å—ñ—Ö –¥–Ω—ñ–≤ –≤—ñ–¥ BASE_DATE –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–ø–∏—Å–∫—É —Å–ª—ñ–≤ –¥–Ω—è
    with open("daily_words.txt", "r", encoding="utf-8") as f:
        daily_words = [line.strip() for line in f if line.strip()]

    with open("wordlist.txt", "r", encoding="utf-8") as f:
        words = [line.strip() for line in f if line.strip()]

    with open("definitions.json", "r", encoding="utf-8") as f:
        definitions = json.load(f)

    for i, target_word in enumerate(daily_words):
        day = BASE_DATE + timedelta(days=i)
        output_file = f"precomputed/{day}.json"
        if os.path.exists(output_file):
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ {target_word} (–≤–∂–µ —ñ—Å–Ω—É—î)")
            continue
        generate_rankings(target_word, day, definitions, words)
