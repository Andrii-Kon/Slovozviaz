import openai
import json
import os
import numpy as np
import time
from tqdm import tqdm
from dotenv import load_dotenv
from datetime import timedelta, date

load_dotenv()

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–º—ñ–Ω–Ω–æ—ó –æ—Ç–æ—á–µ–Ω–Ω—è OPENAI_API_KEY!")

client = openai.OpenAI(api_key=api_key)

CACHE_FILE = "embeddings_cache.json"
BATCH_SIZE = 100
BASE_DATE = date(2025, 5, 11)

if not os.path.exists("precomputed"):
    os.makedirs("precomputed")

# Load or create cache
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        embedding_cache = json.load(f)
else:
    embedding_cache = {}

def get_embeddings_batch(phrases, model="text-embedding-3-large"):
    uncached = [p for p in phrases if p not in embedding_cache]

    if uncached:
        for i in range(0, len(uncached), BATCH_SIZE):
            batch = uncached[i:i + BATCH_SIZE]
            print(f"üîÑ –ù–∞–¥—Å–∏–ª–∞—î–º–æ batch {i + 1}‚Äì{i + len(batch)}")
            response = client.embeddings.create(input=batch, model=model)
            for phrase, obj in zip(batch, response.data):
                embedding_cache[phrase] = obj.embedding

        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(embedding_cache, f, ensure_ascii=False)

    return [embedding_cache[p] for p in phrases]

def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def generate_rankings(target_word, target_date, definitions, words):
    print(f"[{target_date}] ‚ñ∂Ô∏è –û–±—Ä–æ–±–∫–∞ —Å–ª–æ–≤–∞: {target_word}")
    target_phrase = definitions.get(target_word, target_word)
    target_emb = get_embeddings_batch([target_phrase])[0]

    phrases = [definitions.get(w, w) for w in words]
    print("üì• –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥–¥—ñ–Ω–≥–∏ –¥–ª—è –≤—Å—ñ—Ö —Ñ—Ä–∞–∑...")
    embeddings = get_embeddings_batch(phrases)

    scored = []
    for word, emb in tqdm(zip(words, embeddings), total=len(words), desc="–û–±—á–∏—Å–ª—é—î–º–æ —Å—Ö–æ–∂—ñ—Å—Ç—å"):
        sim = cosine_similarity(emb, target_emb)
        scored.append((word, sim))

    scored.sort(key=lambda x: x[1], reverse=True)
    ranked_words = [{"word": w, "similarity": s, "rank": r} for r, (w, s) in enumerate(scored, start=1)]

    filename = f"precomputed/{target_date}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(ranked_words, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —É {filename} ({len(ranked_words)} —Å–ª—ñ–≤)")
    return ranked_words

if __name__ == "__main__":
    with open("daily_words.txt", "r", encoding="utf-8") as f:
        daily_words = [line.strip() for line in f if line.strip()]

    with open("wordlist.txt", "r", encoding="utf-8") as f:
        words = [line.strip() for line in f if line.strip()]

    with open("definitions.json", "r", encoding="utf-8") as f:
        definitions = json.load(f)

    for i, target_word in enumerate(daily_words):
        day = BASE_DATE + timedelta(days=i)
        output_file = f"precomputed/{day}.json"
        if os.path.exists(output_file):
            print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {target_word} (–≤–∂–µ —ñ—Å–Ω—É—î)")
            continue
        generate_rankings(target_word, day, definitions, words)
